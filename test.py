import streamlit as st
import pandas as pd
import asyncio
import os

# Import internes
from app.validation import validate_urls
from app.crawling import crawl_websites
from app.transform import transform_data
from app.visualizer import generate_word_cloud, generate_bar_chart, summarize_statistics

# Configuration Matplotlib pour un backend non-GUI
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
matplotlib.use("Agg")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CONFIGURATION GENERALE STREAMLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title=" Web Data Explorer",
    page_icon="ğŸ’",
    layout="wide",
    initial_sidebar_state="expanded",
)

# CSS personnalisÃ© avec styles avancÃ©s
ADVANCED_CSS = """
<style>
@keyframes fade-in {
  0% {
    opacity: 0;
    transform: translateY(20px);
  }
  100% {
    opacity: 1;
    transform: translateY(0);
  }
}

/* On cible .stApp au lieu de body */
.stApp {
    animation: fade-in 0.8s ease-in-out forwards;
    color: #2C3E50 !important;
}

.block-container {
    animation: fade-in 1s ease-in-out forwards;
    background-color: rgba(255, 255, 255, 0.7);
    padding: 1.5rem 2rem;
    border-radius: 10px;
    box-shadow: 0 12px 20px rgba(0, 0, 0, 0.15);
    margin-top: 1rem;
    margin-bottom: 1rem;
}

/* On laisse le reste des styles : titres, boutons, images, etc. */
h1, h2, h3, h4, h5, h6 {
    text-shadow: 1px 1px 2px rgba(50, 50, 70, 0.4);
}
</style>

"""
st.markdown(ADVANCED_CSS, unsafe_allow_html=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. EN-TÃŠTE DE PAGE (LOGOS + TITRE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
col1, col2, col3 = st.columns([2, 4, 2])
with col1:
    st.image("uploads/logo-UM6P-CC.jpg", width=270)
with col2:
    st.markdown(
        """
        <h1 style='text-align: center; margin-top:40px; font-size: 2.5rem;'>
             Web Data Explorer
        </h1>
        """,
        unsafe_allow_html=True,
    )
with col3:
    st.image("uploads/images.png", width=210)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. VARIABLES ET FONCTIONS UTILES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

def run_async_task(task):
    """
    ExÃ©cute une coroutine asyncio dans Streamlit de maniÃ¨re synchrone.
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    result = loop.run_until_complete(task)
    loop.close()
    return result

# Gestion de l'Ã©tat de la session
if "validated_urls" not in st.session_state:
    st.session_state.validated_urls = []
if "selected_urls" not in st.session_state:
    st.session_state.selected_urls = []
if "processed" not in st.session_state:
    st.session_state.processed = False

# Barre latÃ©rale (instructions & infos)
st.sidebar.title("ğŸ”§ ParamÃ¨tres & Infos")
st.sidebar.markdown(
    """
    **Comment Utiliser :**
    1. **Upload & Validate** : TÃ©lÃ©versez un fichier Excel contenant une colonne 'Website'.
    2. **Crawling Settings** : Choisissez la profondeur, sÃ©lectionnez vos URLs, puis lancez le crawl.
    3. **Analytics & Visuals** : Consultez les rÃ©sultats, les nuages de mots et exportez les donnÃ©es.

    **Astuces :**
    - Augmentez la profondeur pour rÃ©cupÃ©rer plus de liens.
    - VÃ©rifiez vos ressources : trop d'URLs peuvent ralentir l'exÃ©cution.

    **Made with â¤ï¸ by Soufiane**
    """
)

# DÃ©claration des onglets principaux
tab1, tab2, tab3 = st.tabs([
    "ğŸ“‚ Upload & Validate",
    "ğŸŒ Crawling Settings",
    "ğŸ“Š Analytics & Visuals"
])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ONGLET 1 : UPLOAD & VALIDATE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    st.title("ğŸš€ Upload & Validate")
    st.write("TÃ©lÃ©versez un fichier Excel avec une colonne **Website** pour commencer.")

    uploaded_file = st.file_uploader(
        "Upload Your Excel File (avec une colonne 'Website')",
        type=["xlsx"]
    )

    if uploaded_file:
        input_path = os.path.join(UPLOAD_DIR, "uploaded_file.xlsx")
        with open(input_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success("ğŸ“ Fichier tÃ©lÃ©versÃ© avec succÃ¨s !")


        try:
            df = pd.read_excel(uploaded_file)
            st.subheader("ğŸ” AperÃ§u du fichier tÃ©lÃ©versÃ©")
            st.dataframe(df)  # Display the DataFrame
        except Exception as e:
            st.error(f"Erreur lors de la lecture du fichier : {e}")

        # SÃ©lection du nombre dâ€™URLs Ã  valider
        num_urls = st.number_input(
            "Nombre d'URLs Ã  valider",
            min_value=1,
            max_value=1000,
            value=10
        )

        validate_col1, validate_col2 = st.columns([2, 3])
        with validate_col1:
            if st.button("ğŸ”¥ Valider les URLs"):
                st.info("â³ Lancement de la validation des URLs...")
                validated_file = os.path.join(UPLOAD_DIR, "validated_urls.xlsx")
                run_async_task(
                    validate_urls(
                        input_path,
                        validated_file,
                        max_urls=num_urls,
                        max_concurrent_tasks=50
                    )
                )
                st.success(f"âœ… Validation terminÃ©e pour {num_urls} URLs !")
                validated_urls_df = pd.read_excel(validated_file)
                st.session_state.validated_urls = validated_urls_df["URL"].tolist()

        if st.session_state.validated_urls:
            st.subheader("ğŸ” AperÃ§u des URLs validÃ©es")
            with st.expander("Cliquez pour voir la liste", expanded=True):
                st.write(st.session_state.validated_urls)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ONGLET 2 : CRAWLING SETTINGS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab2:
    st.title("ğŸ•µï¸ Crawling Settings")

    max_depth = st.slider(
        "Profondeur maximale (Max Depth)",
        min_value=1,
        max_value=5,
        value=1,
        help="DÃ©finit le nombre de niveaux de liens Ã  explorer."
    )

    if st.session_state.validated_urls:
        # Checkbox to select all URLs
        select_all = st.checkbox(
            "SÃ©lectionner toutes les URLs",
            value=False,
            help="Cochez cette option pour sÃ©lectionner toutes les URLs validÃ©es pour le crawl."
        )

        # Handle URL selection based on the checkbox state
        if select_all:
            st.session_state.selected_urls = st.session_state.validated_urls
        else:
            st.session_state.selected_urls = st.multiselect(
                "SÃ©lectionnez les sites Ã  crawler",
                options=st.session_state.validated_urls,
                default=st.session_state.selected_urls
            )

        if st.session_state.selected_urls:
            st.info(f"ğŸŒ {len(st.session_state.selected_urls)} sites prÃªts pour le crawl.")
            
            # Bouton de dÃ©marrage du crawl
            if st.button("DÃ©marrer le Crawling"):
                st.info("Lancement du Web Crawling...")
                crawled_file = os.path.join(UPLOAD_DIR, "crawled_data.xlsx")
                adjacency_map = run_async_task(
                    crawl_websites(
                        st.session_state.selected_urls,
                        crawled_file,
                        max_depth=max_depth,
                        max_pages=len(st.session_state.selected_urls),
                        max_concurrent_requests=80,  # Ajustable selon vos besoins
                    )
                )
                st.session_state.processed = True
                st.session_state.adjacency_map = adjacency_map
                st.success("Web Crawling terminÃ© !")

                # Visualisation immÃ©diate de la structure (hiÃ©rarchie)
                if "adjacency_map" in st.session_state and st.session_state.adjacency_map:
                    st.subheader("Structure de Crawl")

                    def build_tree_markdown(node, adjacency, level=0):
                        """
                        Construit un arbre en format Markdown
                        pour afficher la hiÃ©rarchie des liens explorÃ©s.
                        """
                        indent = "    " * level
                        md = f"{indent}- {node}\n"
                        children = adjacency.get(node, [])
                        for child in children:
                            md += build_tree_markdown(child, adjacency, level + 1)
                        return md

                    st.write("Ci-dessous la hiÃ©rarchie des pages dÃ©couvertes :")
                    roots = [
                        url for url in st.session_state.selected_urls
                        if url in st.session_state.adjacency_map
                    ]
                    for root in roots:
                        with st.expander(root):
                            children = st.session_state.adjacency_map.get(root, [])
                            if children:
                                tree_md = ""
                                for child in children:
                                    tree_md += build_tree_markdown(child, st.session_state.adjacency_map, 1)
                                st.markdown(tree_md)
                            else:
                                st.write("Aucun lien dÃ©couvert.")
        else:
            st.warning("Veuillez sÃ©lectionner au moins une URL pour lancer le crawl.")
    else:
        st.warning("Aucune URL validÃ©e. Veuillez d'abord passer par l'Ã©tape prÃ©cÃ©dente.")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ONGLET 3 : ANALYTICS & VISUALS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab3:
    st.title("ğŸ“Š Analytics & Visuals")

    if st.session_state.processed:
        crawled_file = os.path.join(UPLOAD_DIR, "crawled_data.xlsx")
        transformed_file = os.path.join(UPLOAD_DIR, "transformed_data.xlsx")

        if os.path.exists(crawled_file):
            df_check = pd.read_excel(crawled_file)
            with st.expander("ğŸ‘€ AperÃ§u des donnÃ©es crawlÃ©es"):
                st.write(df_check.head())

            # VÃ©rification de la colonne "Content"
            if "Content" not in df_check.columns:
                st.warning("Le fichier crawlÃ© ne contient pas de colonne 'Content'. Rien Ã  transformer.")
            else:
                # Transformation des donnÃ©es
                st.info("ğŸ”§ Transformation des donnÃ©es en cours...")
                transform_data(crawled_file, transformed_file)
                st.success("âœ… Transformation terminÃ©e !")

                # GÃ©nÃ©ration des visualisations
                st.info("ğŸ¨ GÃ©nÃ©ration des visualisations...")
                df = pd.read_excel(transformed_file)
                
                wordcloud_path = os.path.join(UPLOAD_DIR, "wordcloud.png")
                barchart_path = os.path.join(UPLOAD_DIR, "barchart.png")
                df["Processed_Content"] = df["Processed_Content"].fillna("").astype(str)
                generate_word_cloud(df["Processed_Content"], wordcloud_path)
                generate_bar_chart(df["Processed_Content"], barchart_path)
                stats = summarize_statistics(df["Processed_Content"])

                # Affichage des rÃ©sultats et graphiques
                st.subheader("ğŸ“ˆ Statistiques")
                st.write(f"**Total d'URLs CrawlÃ©es :** {stats['total_urls']}")
                st.write(f"**Longueur moyenne du Contenu :** {stats['avg_content_length']:.2f} mots")

                col_wc, col_bc = st.columns(2)
                with col_wc:
                    st.subheader("â˜ï¸ Nuage de Mots")
                    st.image(wordcloud_path, use_column_width=True)
                with col_bc:
                    st.subheader("ğŸ” Top 10 Mots")
                    st.image(barchart_path, use_column_width=True)

                                # Feature: Keyword Statistics

                st.subheader("ğŸ” Analyse par Mot-ClÃ©")
                keyword = st.text_input("Entrez un mot-clÃ© pour l'analyse", value="")





                if keyword.strip():
                    keyword = keyword.lower()
                    df["Processed_Content"] = df["Processed_Content"].str.lower()
                    df["Keyword_Occurrences"] = df["Processed_Content"].apply(lambda x: x.count(keyword))
                    
                    # Calculating statistics
                    total_occurrences = df["Keyword_Occurrences"].sum()
                    pages_with_keyword = df[df["Keyword_Occurrences"] > 0]
                    num_pages_with_keyword = len(pages_with_keyword)
                    percentage_pages_with_keyword = (num_pages_with_keyword / len(df)) * 100 if len(df) > 0 else 0




                    col_wc, col_bc = st.columns(2)
                    with col_wc:
                        # Displaying statistics
                        st.write(f"**Mot-ClÃ© :** `{keyword}`")
                        st.write(f"**Nombre total d'occurrences :** {total_occurrences}")
                        st.write(f"**Nombre de pages contenant le mot-clÃ© :** {num_pages_with_keyword}")
                        st.write(f"**Pourcentage de pages contenant le mot-clÃ© :** {percentage_pages_with_keyword:.2f}%")
                    with col_bc:
                        if num_pages_with_keyword > 0:
                            st.subheader("ğŸ“„ Pages contenant le mot-clÃ©")
                            st.dataframe(pages_with_keyword[["URL", "Keyword_Occurrences"]].rename(
                                columns={"URL": "Lien", "Keyword_Occurrences": "Occurrences"}
                            ))



                    # Show pages where the keyword appears
                    if num_pages_with_keyword > 0:

                        # Visualizations
                        st.subheader("ğŸ“Š Visualisations du Mot-ClÃ©")

                        # 1. Bar Chart: Top Pages by Keyword Occurrences
                        st.write("### ğŸ” Pages Principales (Occurrences du Mot-ClÃ©)")
                        fig, ax = plt.subplots(figsize=(10, 6))
                        sns.barplot(
                            data=pages_with_keyword.sort_values(by="Keyword_Occurrences", ascending=False).head(10),
                            x="Keyword_Occurrences", y="URL", ax=ax, palette="viridis"
                        )
                        ax.set_title(f"Top 10 Pages Contenant '{keyword}'", fontsize=16)
                        ax.set_xlabel("Occurrences", fontsize=12)
                        ax.set_ylabel("Pages", fontsize=12)
                        st.pyplot(fig)

                        # 2. Pie Chart: Pages with/without Keyword
                        st.write("### ğŸ“ˆ RÃ©partition des Pages (Avec/Sans le Mot-ClÃ©)")
                        keyword_pie_data = {
                            "Statut": ["Avec Mot-ClÃ©", "Sans Mot-ClÃ©"],
                            "Pages": [num_pages_with_keyword, len(df) - num_pages_with_keyword]
                        }
                        pie_chart = px.pie(
                            keyword_pie_data,
                            values="Pages",
                            names="Statut",
                            title=f"RÃ©partition des Pages Contenant '{keyword}'",
                            color_discrete_sequence=px.colors.sequential.Viridis
                        )
                        st.plotly_chart(pie_chart)

                        # 3. Word Distribution in Relevant Pages
                        st.write("### ğŸ“š Distribution des Mots dans les Pages avec le Mot-ClÃ©")
                        word_data = " ".join(pages_with_keyword["Processed_Content"]).split()
                        word_counts = pd.Series(word_data).value_counts().head(20)
                        fig, ax = plt.subplots(figsize=(12, 6))
                        sns.barplot(x=word_counts.values, y=word_counts.index, ax=ax, palette="mako")
                        ax.set_title(f"Distribution des 20 Mots les Plus FrÃ©quents (Dans les Pages Contenant '{keyword}')", fontsize=16)
                        ax.set_xlabel("Occurrences", fontsize=12)
                        ax.set_ylabel("Mots", fontsize=12)
                        st.pyplot(fig)


                # TÃ©lÃ©chargement du fichier transformÃ©
                st.subheader("ğŸ’¾ TÃ©lÃ©charger les donnÃ©es transformÃ©es")
                st.download_button(
                    label="ğŸ’ Download Processed Data",
                    data=open(transformed_file, "rb").read(),
                    file_name="transformed_data.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
        else:
            st.warning("Fichier de crawl introuvable. Impossible de transformer et d'analyser.")
    else:
        st.warning("Aucune donnÃ©e disponible. Veuillez d'abord lancer le crawling.")

